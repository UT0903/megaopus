{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f2b2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecc5b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import Window as W\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import pandas as pd\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "367770dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.startTime', '1645164611650'),\n",
       " ('spark.kubernetes.authenticate.driver.serviceAccountName', 'spark'),\n",
       " ('spark.master', 'k8s://https://10.122.0.1'),\n",
       " ('spark.kubernetes.container.image',\n",
       "  'docker-registry.netbase.com/de/base-images/spark-py:1.2.0-spark3.1.1-test'),\n",
       " ('spark.hadoop.dfs.namenode.rpc-address.nb.h1', 'h3001.ali-netbase.com:9820'),\n",
       " ('spark.kubernetes.executor.podNamePrefix',\n",
       "  'jupyterhub-user-ali40netbase-com-87ace67f0b73c8c1'),\n",
       " ('spark.kubernetes.executor.label.eci', 'true'),\n",
       " ('spark.kubernetes.pyspark.pythonVersion', '3'),\n",
       " ('spark.kubernetes.executor.limit.cores', '8'),\n",
       " ('spark.hadoop.fs.hdfs.impl', 'org.apache.hadoop.hdfs.DistributedFileSystem'),\n",
       " ('spark.sql.warehouse.dir', 'file:/home/jovyan/megaopus/spark-warehouse'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.default.parallelism', '32'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.id', 'spark-application-1645164611782'),\n",
       " ('spark.hadoop.dfs.namenode.rpc-address.nb.h2', 'h3101.ali-netbase.com:9820'),\n",
       " ('spark.kubernetes.container.image.pullPolicy', 'Always'),\n",
       " ('spark.executor.cores', '8'),\n",
       " ('spark.executorEnv.HADOOP_USER_NAME', 'woot'),\n",
       " ('spark.executor.memory', '24g'),\n",
       " ('spark.driver.port', '38499'),\n",
       " ('spark.driver.host', '172.25.16.29'),\n",
       " ('spark.hadoop.fs.defaultFS', 'hdfs://nb:9820'),\n",
       " ('spark.kubernetes.executorEnv.HADOOP_USER_NAME', 'woot'),\n",
       " ('spark.kubernetes.executor.request.cores', '8'),\n",
       " ('spark.kubernetes.allocation.batch.size', '8'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.executor.instances', '8'),\n",
       " ('spark.hadoop.dfs.ha.namenodes.nb', 'h1,h2'),\n",
       " ('spark.driver.cores', '16'),\n",
       " ('spark.hadoop.conf', 'org.apache.hadoop.hdfs.HdfsConfiguration'),\n",
       " ('spark.kubernetes.executor.annotation.k8s.aliyun.com/eci-use-specs',\n",
       "  '8-32Gi'),\n",
       " ('spark.app.name', 'jupyterhub-user-ali%40netbase.com'),\n",
       " ('spark.hadoop.dfs.client.failover.proxy.provider.nb',\n",
       "  'org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider'),\n",
       " ('spark.kubernetes.namespace', 'jupyterhub'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.hadoop.fs.hdfs.server',\n",
       "  'org.apache.hadoop.hdfs.server.namenode.NameNode'),\n",
       " ('spark.hadoop.dfs.nameservices', 'nb'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.memory', '128g')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf= pyspark.SparkConf()\n",
    "conf.setMaster(\"k8s://https://\"+os.environ['KUBERNETES_SERVICE_HOST'])\n",
    " \n",
    "# For hdfs configuration\n",
    "conf.set(\"spark.hadoop.fs.hdfs.impl\", \"org.apache.hadoop.hdfs.DistributedFileSystem\")\n",
    "conf.set(\"spark.hadoop.fs.hdfs.server\", \"org.apache.hadoop.hdfs.server.namenode.NameNode\")\n",
    "conf.set(\"spark.hadoop.conf\", \"org.apache.hadoop.hdfs.HdfsConfiguration\")\n",
    "conf.set(\"spark.hadoop.dfs.nameservices\", \"nb\")\n",
    "conf.set(\"spark.hadoop.dfs.ha.namenodes.nb\", \"h1,h2\")\n",
    "conf.set(\"spark.hadoop.dfs.namenode.rpc-address.nb.h1\", \"h3001.ali-netbase.com:9820\")\n",
    "conf.set(\"spark.hadoop.dfs.namenode.rpc-address.nb.h2\", \"h3101.ali-netbase.com:9820\")\n",
    "conf.set(\"spark.hadoop.dfs.client.failover.proxy.provider.nb\", \"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\")\n",
    "conf.set(\"spark.hadoop.fs.defaultFS\", \"hdfs://nb:9820\")\n",
    " \n",
    "# For kubernetes configration\n",
    "f_name = \"big_data.csv\"\n",
    "executor_cores=8\n",
    "executor_machine_memory=32\n",
    "executor_heap_memory=24\n",
    "executor_instances=8\n",
    "driver_cores=16\n",
    "driver_memory=128\n",
    " \n",
    "# request on-demand machines\n",
    "conf.set(\"spark.kubernetes.executor.label.eci\", \"true\")\n",
    "conf.set(\"spark.kubernetes.executor.annotation.k8s.aliyun.com/eci-use-specs\", \"{}-{}Gi\".format(executor_cores, executor_machine_memory))\n",
    "  \n",
    "# machine numbers\n",
    "conf.set(\"spark.executor.instances\", executor_instances)\n",
    "conf.set(\"spark.kubernetes.allocation.batch.size\", executor_instances)\n",
    " \n",
    "  \n",
    "# machine cpu numbers\n",
    "conf.set(\"spark.kubernetes.executor.request.cores\", executor_cores)\n",
    "conf.set(\"spark.kubernetes.executor.limit.cores\", executor_cores)\n",
    "conf.set(\"spark.executor.memory\", \"{}g\".format(executor_heap_memory))\n",
    "conf.set(\"spark.executor.cores\", executor_cores)\n",
    "conf.set(\"spark.driver.memory\", \"{}g\".format(driver_memory))\n",
    "conf.set(\"spark.driver.cores\", driver_cores)\n",
    "  \n",
    "# spark version\n",
    "# conf.set(\"spark.kubernetes.container.image\", \"maven-docker.netbase.com/spark-py:v2.4.6\")\n",
    "# conf.set(\"spark.kubernetes.container.image\", \"docker-registry.netbase.com/de/base-images/spark-py:1.2.0-spark3.1.1\")\n",
    "conf.set(\"spark.kubernetes.container.image\", \"docker-registry.netbase.com/de/base-images/spark-py:1.2.0-spark3.1.1-test\")\n",
    "conf.set(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
    "  \n",
    "conf.set(\"spark.kubernetes.namespace\", \"jupyterhub\")\n",
    "conf.set(\"spark.driver.host\", os.environ['HOSTIP'])\n",
    "conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "conf.set('spark.submit.deployMode', 'client')\n",
    "conf.set('spark.kubernetes.pyspark.pythonVersion', \"3\")\n",
    "# tell spark executor act on behalf to user woot\n",
    "conf.set('spark.kubernetes.executorEnv.HADOOP_USER_NAME', \"woot\")\n",
    "conf.set('spark.executorEnv.HADOOP_USER_NAME', \"woot\")\n",
    " \n",
    "conf.set('spark.default.parallelism', '32')\n",
    "conf.setAppName( os.environ['JUPYTERHUB_CLIENT_ID'])\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "   \n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65d6acf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "path = 'hdfs://nb/ai-pipeline/megaopus_data/{}'.format(f_name)\n",
    "_df = spark.read.options(header=True, encoding=\"UTF-8\").csv(path)\n",
    "_df = _df[_df['Sound Bite Text'].isNotNull()]\n",
    "_df = _df.select(F.split(_df['Sound Bite Text'], \" \").alias('word_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3ac34908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|          word_array|  n|\n",
      "+--------------------+---+\n",
      "|[#ESG, News:, Som...|  0|\n",
      "|[#ESG, News:, Som...|  1|\n",
      "|[#ESG, News:, Som...|  2|\n",
      "|[#ESG, News:, Som...|  3|\n",
      "|[#ESG, News:, Som...|  4|\n",
      "|[#ESG, News:, Som...|  5|\n",
      "|[#ESG, News:, Som...|  6|\n",
      "|[#ESG, News:, Som...|  7|\n",
      "|[#ESG, News:, Som...|  8|\n",
      "|[#ESG, News:, Som...|  9|\n",
      "|[#ESG, News:, Som...| 10|\n",
      "|[#ESG, News:, Som...| 11|\n",
      "|[#ESG, News:, Som...| 12|\n",
      "|[#ESG, News:, Som...| 13|\n",
      "|[#ESG, News:, Som...| 14|\n",
      "|[#ESG, News:, Som...| 15|\n",
      "|[#ESG, News:, Som...| 16|\n",
      "|[#ESG, News:, Som...| 17|\n",
      "|[#ESG, News:, Som...| 18|\n",
      "|[#ESG, News:, Som...| 19|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "132118200"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_TIME = 150\n",
    "if repeat_time >= 1:\n",
    "    n_to_array = F.udf(lambda n : list(range(n)), T.ArrayType(T.IntegerType()))\n",
    "    __df = _df.withColumn('n', F.lit(repeat_time))\n",
    "    __df = __df.withColumn('n', F.explode(n_to_array(__df.n)))\n",
    "else:\n",
    "    __df = _df.limit(int(_df.count() * repeat_time))\n",
    "__df.show()\n",
    "__df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f112385a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word_array: array<string>, n: int]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__df.rdd.getNumPartitions()\n",
    "__df.repartition(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b844eeb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word_array: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- n: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "__df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "68ac6050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(100000, {585: 1.0, 4705: 1.0, 5145: 1.0, 7871: 1.0, 10266: 1.0, 13956: 1.0, 16017: 3.0, 21318: 1.0, 25209: 1.0, 26365: 1.0, 26467: 1.0, 30488: 1.0, 30578: 1.0, 31833: 1.0, 35578: 1.0, 44658: 1.0, 48540: 1.0, 51152: 1.0, 51299: 1.0, 52276: 1.0, 52671: 1.0, 55867: 1.0, 56721: 1.0, 59189: 1.0, 60364: 1.0, 62891: 1.0, 65581: 2.0, 69218: 1.0, 71266: 1.0, 75882: 1.0, 81991: 1.0, 85726: 1.0, 85870: 1.0, 90073: 1.0, 95553: 1.0, 99737: 1.0})"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashingTF = pyspark.ml.feature.HashingTF(inputCol=\"word_array\", outputCol=\"features\")\n",
    "hashingTF.setNumFeatures(100000)\n",
    "tf = hashingTF.transform(__df)\n",
    "tf.head().features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "18b90d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 822:=====================================================> (31 + 1) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word_array: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- n: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- idf: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[word_array: array<string>, n: int, features: vector, idf: vector]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cache()\n",
    "idf_model = pyspark.ml.feature.IDF().setInputCol(\"features\").setOutputCol(\"idf\").fit(tf)\n",
    "tf_idf = idf_model.transform(tf)\n",
    "tf_idf.printSchema()\n",
    "tf_idf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abb4fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/18 06:58:48 WARN DAGScheduler: Broadcasting large task binary with size 1638.4 KiB\n",
      "22/02/18 06:59:38 WARN DAGScheduler: Broadcasting large task binary with size 1639.0 KiB\n",
      "22/02/18 06:59:48 WARN DAGScheduler: Broadcasting large task binary with size 1639.7 KiB\n",
      "22/02/18 07:00:09 WARN DAGScheduler: Broadcasting large task binary with size 1640.0 KiB\n",
      "22/02/18 07:00:22 WARN DAGScheduler: Broadcasting large task binary with size 1640.0 KiB\n",
      "22/02/18 07:00:38 WARN DAGScheduler: Broadcasting large task binary with size 1640.4 KiB\n",
      "22/02/18 07:00:49 WARN DAGScheduler: Broadcasting large task binary with size 1640.4 KiB\n",
      "22/02/18 07:01:23 WARN DAGScheduler: Broadcasting large task binary with size 1640.7 KiB\n",
      "22/02/18 07:01:34 WARN DAGScheduler: Broadcasting large task binary with size 1640.7 KiB\n",
      "22/02/18 07:01:48 WARN DAGScheduler: Broadcasting large task binary with size 1641.1 KiB\n",
      "22/02/18 07:01:59 WARN DAGScheduler: Broadcasting large task binary with size 1641.1 KiB\n",
      "22/02/18 07:02:14 WARN DAGScheduler: Broadcasting large task binary with size 1641.4 KiB\n",
      "22/02/18 07:02:25 WARN DAGScheduler: Broadcasting large task binary with size 1641.4 KiB\n",
      "22/02/18 07:02:41 WARN DAGScheduler: Broadcasting large task binary with size 1641.8 KiB\n",
      "22/02/18 07:02:51 WARN DAGScheduler: Broadcasting large task binary with size 1641.8 KiB\n",
      "22/02/18 07:03:06 WARN DAGScheduler: Broadcasting large task binary with size 1642.1 KiB\n",
      "22/02/18 07:03:17 WARN DAGScheduler: Broadcasting large task binary with size 1642.1 KiB\n",
      "22/02/18 07:03:33 WARN DAGScheduler: Broadcasting large task binary with size 1642.5 KiB\n",
      "22/02/18 07:03:43 WARN DAGScheduler: Broadcasting large task binary with size 1642.5 KiB\n",
      "22/02/18 07:03:58 WARN DAGScheduler: Broadcasting large task binary with size 1642.9 KiB\n",
      "22/02/18 07:04:09 WARN DAGScheduler: Broadcasting large task binary with size 1642.9 KiB\n",
      "22/02/18 07:04:23 WARN DAGScheduler: Broadcasting large task binary with size 1643.2 KiB\n",
      "22/02/18 07:04:34 WARN DAGScheduler: Broadcasting large task binary with size 1641.0 KiB\n",
      "[Stage 846:=====================================================> (31 + 1) / 32]\r"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "ta = datetime.now()\n",
    "kmeans = KMeans(featuresCol='idf', k=5, initSteps=10).setSeed(1)\n",
    "model = kmeans.fit(tf_idf)\n",
    "predictions = model.transform(tf_idf)\n",
    "td = datetime.now()\n",
    "print('elapsed time:', (td - ta).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d73a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3745a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ClusteringEvaluator(predictionCol='prediction', featuresCol='features', \\\n",
    "        metricName='silhouette', distanceMeasure='squaredEuclidean')\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ae145",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.groupBy('prediction').\\\n",
    "        count().\\\n",
    "        sort(F.desc('count')).\\\n",
    "        show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "642c3031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark311_py39",
   "language": "python",
   "name": "spark311_py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
